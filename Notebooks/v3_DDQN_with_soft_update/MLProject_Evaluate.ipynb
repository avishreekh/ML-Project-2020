{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import huber_loss\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments\n",
    "The following three cells correspond to the environments provided for tasks 1, 2 and 3 respectively. The environment classes are labelled CartPoleEnv1 (task 1), CartPoleEnv2 (task 2) and CartPoleEnv3(task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment for Task 1\n",
    "class CartPoleEnv1(gym.Env):\n",
    "\tmetadata = {\n",
    "\t\t\t'render.modes': ['human', 'rgb_array'],\n",
    "\t\t\t'video.frames_per_second' : 50\n",
    "\t}\n",
    "\n",
    "\tdef __init__(self,case=1):\n",
    "\t\tself.__version__ = \"0.2.0\"\n",
    "\t\tprint(\"CartPoleEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n",
    "\t\tself.gravity = 9.8\n",
    "\t\tself.masscart = 1.0\n",
    "\t\tself.masspole = 0.4\n",
    "\t\tself.total_mass = (self.masspole + self.masscart)\n",
    "\t\tself.length = 0.5 \n",
    "\t\tself.polemass_length = (self.masspole * self.length)\n",
    "\t\tself._seed()\n",
    "\n",
    "\t\tself.force_mag = 10.0\n",
    "\t\t#self.force_mag = 10.0*(1+self.np_random.uniform(low=-0.10, high=0.10))\n",
    "\n",
    "\t\t \n",
    "\t\tself.tau = 0.02  # seconds between state updates\n",
    "\t\tself.frictioncart = 5e-4 # AA Added cart friction\n",
    "\t\tself.frictionpole = 2e-6 # AA Added cart friction\n",
    "\t\tself.gravity_eps = 0.99 # Random scaling for gravity\n",
    "\t\tself.frictioncart_eps = 0.99 # Random scaling for friction\n",
    "\t\tself.frictionpole_eps = 0.99 # Random scaling for friction\n",
    "\n",
    "\t\t# Angle at which to fail the episode\n",
    "\t\tself.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "\t\tself.x_threshold = 2.4\n",
    "\n",
    "\t\t# Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "\t\thigh = np.array([\n",
    "\t\t\tself.x_threshold * 2,\n",
    "\t\t\tnp.finfo(np.float32).max,\n",
    "\t\t\tself.theta_threshold_radians * 2,\n",
    "\t\t\tnp.finfo(np.float32).max])\n",
    "\n",
    "\t\tself.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n",
    "\t\tself.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "\t\tself.viewer = None\n",
    "\t\tself.state = None\n",
    "\n",
    "\t\tself.steps_beyond_done = None\n",
    "\n",
    "\tdef _seed(self, seed=None): # Set appropriate seed value\n",
    "\t\tself.np_random, seed = seeding.np_random(seed)\n",
    "\t\treturn [seed]\n",
    "\n",
    "\tdef _step(self, action):\n",
    "\t\tassert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "\t\tstate = self.state\n",
    "\t\tx, x_dot, theta, theta_dot = state\n",
    "\t\tforce = self.force_mag if action==1 else -self.force_mag\n",
    "\t\tcostheta = math.cos(theta)\n",
    "\t\tsintheta = math.sin(theta)\n",
    "\t\ttemp = (force + self.polemass_length * theta_dot * theta_dot * sintheta - self.frictioncart * (4 + self.frictioncart_eps*np.random.randn()) *np.sign(x_dot)) / self.total_mass # AA Added cart friction\n",
    "\t\tthetaacc = (self.gravity * (4 + self.gravity_eps*np.random.randn()) * sintheta - costheta* temp - self.frictionpole * (4 + self.frictionpole_eps*np.random.randn()) *theta_dot/self.polemass_length) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass)) # AA Added pole friction\n",
    "\t\txacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\t\tnoise = 0\n",
    "\t\t#noise = self.np_random.uniform(low=-0.10, high=0.10) \n",
    "\t\tx  = (x + self.tau * x_dot)\n",
    "\t\tx_dot = (x_dot + self.tau * xacc)\n",
    "\t\ttheta = (theta + self.tau * theta_dot)*(1 + noise)\n",
    "\t\ttheta_dot = (theta_dot + self.tau * thetaacc)\n",
    "\t\tself.state = (x,x_dot,theta,theta_dot)\n",
    "\t\tdone =  x < -self.x_threshold \\\n",
    "\t\t\t\tor x > self.x_threshold \\\n",
    "\t\t\t\tor theta < -self.theta_threshold_radians \\\n",
    "\t\t\t\tor theta > self.theta_threshold_radians\n",
    "\t\tdone = bool(done)\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\treward = 1.0\n",
    "\t\telif self.steps_beyond_done is None:\n",
    "\t\t\t# Pole just fell!\n",
    "\t\t\tself.steps_beyond_done = 0\n",
    "\t\t\treward = 1.0\n",
    "\t\telse:\n",
    "\t\t\tif self.steps_beyond_done == 0:\n",
    "\t\t\t\tlogger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "\t\t\tself.steps_beyond_done += 1\n",
    "\t\t\treward = 0.0\n",
    "\n",
    "\t\treturn np.array(self.state), reward, done, {}\n",
    "\n",
    "\tdef _reset(self):\n",
    "\t\tself.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "\t\tself.steps_beyond_done = None\n",
    "\t\treturn np.array(self.state)\n",
    "\n",
    "\tdef _render(self, mode='human', close=False):\n",
    "\t\tif close:\n",
    "\t\t\tif self.viewer is not None:\n",
    "\t\t\t\tself.viewer.close()\n",
    "\t\t\t\tself.viewer = None\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tscreen_width = 600\n",
    "\t\tscreen_height = 400\n",
    "\n",
    "\t\tworld_width = self.x_threshold*2\n",
    "\t\tscale = screen_width/world_width\n",
    "\t\tcarty = 100 # TOP OF CART\n",
    "\t\tpolewidth = 10.0\n",
    "\t\tpolelen = scale * 1.0\n",
    "\t\tcartwidth = 50.0\n",
    "\t\tcartheight = 30.0\n",
    "\n",
    "\t\tif self.viewer is None:\n",
    "\t\t\tfrom gym.envs.classic_control import rendering\n",
    "\t\t\tself.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "\t\t\tl,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "\t\t\taxleoffset =cartheight/4.0\n",
    "\t\t\tcart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tself.carttrans = rendering.Transform()\n",
    "\t\t\tcart.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(cart)\n",
    "\t\t\tl,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "\t\t\tpole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tpole.set_color(.8,.6,.4)\n",
    "\t\t\tself.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "\t\t\tpole.add_attr(self.poletrans)\n",
    "\t\t\tpole.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(pole)\n",
    "\t\t\tself.axle = rendering.make_circle(polewidth/2)\n",
    "\t\t\tself.axle.add_attr(self.poletrans)\n",
    "\t\t\tself.axle.add_attr(self.carttrans)\n",
    "\t\t\tself.axle.set_color(.5,.5,.8)\n",
    "\t\t\tself.viewer.add_geom(self.axle)\n",
    "\t\t\tself.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "\t\t\tself.track.set_color(0,0,0)\n",
    "\t\t\tself.viewer.add_geom(self.track)\n",
    "\n",
    "\t\tif self.state is None: return None\n",
    "\n",
    "\t\tx = self.state\n",
    "\t\tcartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "\t\tself.carttrans.set_translation(cartx, carty)\n",
    "\t\tself.poletrans.set_rotation(-x[2])\n",
    "\t\treturn self.viewer.render(return_rgb_array = mode=='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment for Task 2\n",
    "class CartPoleEnv2(gym.Env):\n",
    "\tmetadata = {\n",
    "\t\t\t'render.modes': ['human', 'rgb_array'],\n",
    "\t\t\t'video.frames_per_second' : 50\n",
    "\t}\n",
    "\n",
    "\tdef __init__(self,case=1):\n",
    "\t\tself.__version__ = \"0.2.0\"\n",
    "\t\tprint(\"CartPoleEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n",
    "\t\tself.gravity = 9.8\n",
    "\t\tself.masscart = 1.0\n",
    "\t\tself.masspole = 0.4\n",
    "\t\tself.total_mass = (self.masspole + self.masscart)\n",
    "\t\tself.length = 0.5 \n",
    "\t\tself.polemass_length = (self.masspole * self.length)\n",
    "\t\tself._seed()\n",
    "\n",
    "\t\t#self.force_mag = 10.0\n",
    "\t\tself.force_mag = 10.0*(1+self.np_random.uniform(low=-0.30, high=0.30))\n",
    "\n",
    "\t\t \n",
    "\t\tself.tau = 0.02  # seconds between state updates\n",
    "\t\tself.frictioncart = 5e-4 # AA Added cart friction\n",
    "\t\tself.frictionpole = 2e-6 # AA Added cart friction\n",
    "\t\tself.gravity_eps = 0.99 # Random scaling for gravity\n",
    "\t\tself.frictioncart_eps = 0.99 # Random scaling for friction\n",
    "\t\tself.frictionpole_eps = 0.99 # Random scaling for friction\n",
    "\n",
    "\t\t# Angle at which to fail the episode\n",
    "\t\tself.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "\t\tself.x_threshold = 2.4\n",
    "\n",
    "\t\t# Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "\t\thigh = np.array([\n",
    "\t\t\tself.x_threshold * 2,\n",
    "\t\t\tnp.finfo(np.float32).max,\n",
    "\t\t\tself.theta_threshold_radians * 2,\n",
    "\t\t\tnp.finfo(np.float32).max])\n",
    "\n",
    "\t\tself.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n",
    "\t\tself.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "\t\tself.viewer = None\n",
    "\t\tself.state = None\n",
    "\n",
    "\t\tself.steps_beyond_done = None\n",
    "\n",
    "\tdef _seed(self, seed=None): # Set appropriate seed value\n",
    "\t\tself.np_random, seed = seeding.np_random(seed)\n",
    "\t\treturn [seed]\n",
    "\n",
    "\tdef _step(self, action):\n",
    "\t\tassert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "\t\tstate = self.state\n",
    "\t\tx, x_dot, theta, theta_dot = state\n",
    "\t\tforce = self.force_mag if action==1 else -self.force_mag\n",
    "\t\tcostheta = math.cos(theta)\n",
    "\t\tsintheta = math.sin(theta)\n",
    "\t\ttemp = (force + self.polemass_length * theta_dot * theta_dot * sintheta - self.frictioncart * (4 + self.frictioncart_eps*np.random.randn()) *np.sign(x_dot)) / self.total_mass # AA Added cart friction\n",
    "\t\tthetaacc = (self.gravity * (4 + self.gravity_eps*np.random.randn()) * sintheta - costheta* temp - self.frictionpole * (4 + self.frictionpole_eps*np.random.randn()) *theta_dot/self.polemass_length) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass)) # AA Added pole friction\n",
    "\t\txacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\t\tnoise = 0\n",
    "\t\t#noise = self.np_random.uniform(low=-0.10, high=0.10) \n",
    "\t\tx  = (x + self.tau * x_dot)\n",
    "\t\tx_dot = (x_dot + self.tau * xacc)\n",
    "\t\ttheta = (theta + self.tau * theta_dot)*(1 + noise)\n",
    "\t\ttheta_dot = (theta_dot + self.tau * thetaacc)\n",
    "\t\tself.state = (x,x_dot,theta,theta_dot)\n",
    "\t\tdone =  x < -self.x_threshold \\\n",
    "\t\t\t\tor x > self.x_threshold \\\n",
    "\t\t\t\tor theta < -self.theta_threshold_radians \\\n",
    "\t\t\t\tor theta > self.theta_threshold_radians\n",
    "\t\tdone = bool(done)\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\treward = 1.0\n",
    "\t\telif self.steps_beyond_done is None:\n",
    "\t\t\t# Pole just fell!\n",
    "\t\t\tself.steps_beyond_done = 0\n",
    "\t\t\treward = 1.0\n",
    "\t\telse:\n",
    "\t\t\tif self.steps_beyond_done == 0:\n",
    "\t\t\t\tlogger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "\t\t\tself.steps_beyond_done += 1\n",
    "\t\t\treward = 0.0\n",
    "\n",
    "\t\treturn np.array(self.state), reward, done, {}\n",
    "\n",
    "\tdef _reset(self):\n",
    "\t\tself.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "\t\tself.steps_beyond_done = None\n",
    "\t\treturn np.array(self.state)\n",
    "\n",
    "\tdef _render(self, mode='human', close=False):\n",
    "\t\tif close:\n",
    "\t\t\tif self.viewer is not None:\n",
    "\t\t\t\tself.viewer.close()\n",
    "\t\t\t\tself.viewer = None\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tscreen_width = 600\n",
    "\t\tscreen_height = 400\n",
    "\n",
    "\t\tworld_width = self.x_threshold*2\n",
    "\t\tscale = screen_width/world_width\n",
    "\t\tcarty = 100 # TOP OF CART\n",
    "\t\tpolewidth = 10.0\n",
    "\t\tpolelen = scale * 1.0\n",
    "\t\tcartwidth = 50.0\n",
    "\t\tcartheight = 30.0\n",
    "\n",
    "\t\tif self.viewer is None:\n",
    "\t\t\tfrom gym.envs.classic_control import rendering\n",
    "\t\t\tself.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "\t\t\tl,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "\t\t\taxleoffset =cartheight/4.0\n",
    "\t\t\tcart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tself.carttrans = rendering.Transform()\n",
    "\t\t\tcart.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(cart)\n",
    "\t\t\tl,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "\t\t\tpole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tpole.set_color(.8,.6,.4)\n",
    "\t\t\tself.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "\t\t\tpole.add_attr(self.poletrans)\n",
    "\t\t\tpole.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(pole)\n",
    "\t\t\tself.axle = rendering.make_circle(polewidth/2)\n",
    "\t\t\tself.axle.add_attr(self.poletrans)\n",
    "\t\t\tself.axle.add_attr(self.carttrans)\n",
    "\t\t\tself.axle.set_color(.5,.5,.8)\n",
    "\t\t\tself.viewer.add_geom(self.axle)\n",
    "\t\t\tself.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "\t\t\tself.track.set_color(0,0,0)\n",
    "\t\t\tself.viewer.add_geom(self.track)\n",
    "\n",
    "\t\tif self.state is None: return None\n",
    "\n",
    "\t\tx = self.state\n",
    "\t\tcartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "\t\tself.carttrans.set_translation(cartx, carty)\n",
    "\t\tself.poletrans.set_rotation(-x[2])\n",
    "\t\treturn self.viewer.render(return_rgb_array = mode=='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment for Task 3\n",
    "class CartPoleEnv3(gym.Env):\n",
    "\tmetadata = {\n",
    "\t\t\t'render.modes': ['human', 'rgb_array'],\n",
    "\t\t\t'video.frames_per_second' : 50\n",
    "\t}\n",
    "\n",
    "\tdef __init__(self,case=1):\n",
    "\t\tself.__version__ = \"0.2.0\"\n",
    "\t\tprint(\"CartPoleEnv - Version {}, Noise case: {}\".format(self.__version__,case))\n",
    "\t\tself.gravity = 9.8\n",
    "\t\tself.masscart = 1.0\n",
    "\t\tself.masspole = 0.4\n",
    "\t\tself.total_mass = (self.masspole + self.masscart)\n",
    "\t\tself.length = 0.5 \n",
    "\t\tself.polemass_length = (self.masspole * self.length)\n",
    "\t\tself._seed()\n",
    "\n",
    "\t\t#self.force_mag = 10.0\n",
    "\t\tself.force_mag = 10.0*(1+self.np_random.uniform(low=-0.30, high=0.30))\n",
    "\n",
    "\t\t \n",
    "\t\tself.tau = 0.02  # seconds between state updates\n",
    "\t\tself.frictioncart = 5e-4 # AA Added cart friction\n",
    "\t\tself.frictionpole = 2e-6 # AA Added cart friction\n",
    "\t\tself.gravity_eps = 0.99 # Random scaling for gravity\n",
    "\t\tself.frictioncart_eps = 0.99 # Random scaling for friction\n",
    "\t\tself.frictionpole_eps = 0.99 # Random scaling for friction\n",
    "\n",
    "\t\t# Angle at which to fail the episode\n",
    "\t\tself.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "\t\tself.x_threshold = 2.4\n",
    "\n",
    "\t\t# Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "\t\thigh = np.array([\n",
    "\t\t\tself.x_threshold * 2,\n",
    "\t\t\tnp.finfo(np.float32).max,\n",
    "\t\t\tself.theta_threshold_radians * 2,\n",
    "\t\t\tnp.finfo(np.float32).max])\n",
    "\n",
    "\t\tself.action_space = spaces.Discrete(2) # AA Set discrete states back to 2\n",
    "\t\tself.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "\t\tself.viewer = None\n",
    "\t\tself.state = None\n",
    "\n",
    "\t\tself.steps_beyond_done = None\n",
    "\n",
    "\tdef _seed(self, seed=None): # Set appropriate seed value\n",
    "\t\tself.np_random, seed = seeding.np_random(seed)\n",
    "\t\treturn [seed]\n",
    "\n",
    "\tdef _step(self, action):\n",
    "\t\tassert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "\t\tstate = self.state\n",
    "\t\tx, x_dot, theta, theta_dot = state\n",
    "\t\tforce = self.force_mag if action==1 else -self.force_mag\n",
    "\t\tcostheta = math.cos(theta)\n",
    "\t\tsintheta = math.sin(theta)\n",
    "\t\ttemp = (force + self.polemass_length * theta_dot * theta_dot * sintheta - self.frictioncart * (4 + self.frictioncart_eps*np.random.randn()) *np.sign(x_dot)) / self.total_mass # AA Added cart friction\n",
    "\t\tthetaacc = (self.gravity * (4 + self.gravity_eps*np.random.randn()) * sintheta - costheta* temp - self.frictionpole * (4 + self.frictionpole_eps*np.random.randn()) *theta_dot/self.polemass_length) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass)) # AA Added pole friction\n",
    "\t\txacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\t\t#noise = 0\n",
    "\t\tnoise = self.np_random.uniform(low=-0.30, high=0.30) \n",
    "\t\tx  = (x + self.tau * x_dot)\n",
    "\t\tx_dot = (x_dot + self.tau * xacc)\n",
    "\t\ttheta = (theta + self.tau * theta_dot)*(1 + noise)\n",
    "\t\ttheta_dot = (theta_dot + self.tau * thetaacc)\n",
    "\t\tself.state = (x,x_dot,theta,theta_dot)\n",
    "\t\tdone =  x < -self.x_threshold \\\n",
    "\t\t\t\tor x > self.x_threshold \\\n",
    "\t\t\t\tor theta < -self.theta_threshold_radians \\\n",
    "\t\t\t\tor theta > self.theta_threshold_radians\n",
    "\t\tdone = bool(done)\n",
    "\n",
    "\t\tif not done:\n",
    "\t\t\treward = 1.0\n",
    "\t\telif self.steps_beyond_done is None:\n",
    "\t\t\t# Pole just fell!\n",
    "\t\t\tself.steps_beyond_done = 0\n",
    "\t\t\treward = 1.0\n",
    "\t\telse:\n",
    "\t\t\tif self.steps_beyond_done == 0:\n",
    "\t\t\t\tlogger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "\t\t\tself.steps_beyond_done += 1\n",
    "\t\t\treward = 0.0\n",
    "\n",
    "\t\treturn np.array(self.state), reward, done, {}\n",
    "\n",
    "\tdef _reset(self):\n",
    "\t\tself.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "\t\tself.steps_beyond_done = None\n",
    "\t\treturn np.array(self.state)\n",
    "\n",
    "\tdef _render(self, mode='human', close=False):\n",
    "\t\tif close:\n",
    "\t\t\tif self.viewer is not None:\n",
    "\t\t\t\tself.viewer.close()\n",
    "\t\t\t\tself.viewer = None\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tscreen_width = 600\n",
    "\t\tscreen_height = 400\n",
    "\n",
    "\t\tworld_width = self.x_threshold*2\n",
    "\t\tscale = screen_width/world_width\n",
    "\t\tcarty = 100 # TOP OF CART\n",
    "\t\tpolewidth = 10.0\n",
    "\t\tpolelen = scale * 1.0\n",
    "\t\tcartwidth = 50.0\n",
    "\t\tcartheight = 30.0\n",
    "\n",
    "\t\tif self.viewer is None:\n",
    "\t\t\tfrom gym.envs.classic_control import rendering\n",
    "\t\t\tself.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "\t\t\tl,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "\t\t\taxleoffset =cartheight/4.0\n",
    "\t\t\tcart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tself.carttrans = rendering.Transform()\n",
    "\t\t\tcart.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(cart)\n",
    "\t\t\tl,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "\t\t\tpole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "\t\t\tpole.set_color(.8,.6,.4)\n",
    "\t\t\tself.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "\t\t\tpole.add_attr(self.poletrans)\n",
    "\t\t\tpole.add_attr(self.carttrans)\n",
    "\t\t\tself.viewer.add_geom(pole)\n",
    "\t\t\tself.axle = rendering.make_circle(polewidth/2)\n",
    "\t\t\tself.axle.add_attr(self.poletrans)\n",
    "\t\t\tself.axle.add_attr(self.carttrans)\n",
    "\t\t\tself.axle.set_color(.5,.5,.8)\n",
    "\t\t\tself.viewer.add_geom(self.axle)\n",
    "\t\t\tself.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "\t\t\tself.track.set_color(0,0,0)\n",
    "\t\t\tself.viewer.add_geom(self.track)\n",
    "\n",
    "\t\tif self.state is None: return None\n",
    "\n",
    "\t\tx = self.state\n",
    "\t\tcartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "\t\tself.carttrans.set_translation(cartx, carty)\n",
    "\t\tself.poletrans.set_rotation(-x[2])\n",
    "\t\treturn self.viewer.render(return_rgb_array = mode=='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The following cell contains code for evaluating the given models which were written using Keras. \n",
    "1. The number of episodes can be set using the parameter `num_episodes` (default value = 100). \n",
    "2. The number of timesteps per episode can be set using the parameter `num_timesteps` (default value = 500). \n",
    "3. The path of the model can be set using the parameter `model_path` (default path is the model in the current directory). \n",
    "4. If the environment needs to be rendered, the variable `render` should be set to True (default value = False).\n",
    "5. The task can be chosen using the `env` variable (CartPoleEnv1, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleEnv - Version 0.2.0, Noise case: 1\n",
      "Model: \"CP_task3_DDQN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 50\n",
      "Trainable params: 50\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Running the agent for 100 episodes...\n",
      "Mean over 100 episodes: 500.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "num_timesteps = 500\n",
    "model_path = './team10.h5'\n",
    "render = False\n",
    "\n",
    "env = CartPoleEnv1()\n",
    "# env = CartPoleEnv2()\n",
    "# env = CartPoleEnv3()\n",
    "\n",
    "model = load_model(model_path)\n",
    "scores_arr = []\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('\\n\\n\\nRunning the agent for {} episodes...'.format(num_episodes))\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    ep_reward = 0\n",
    "    obs = env._reset()\n",
    "    for t in range(num_timesteps):\n",
    "        if render:\n",
    "            env._render()\n",
    "        obs = np.reshape(obs, (1, 4))\n",
    "        act = np.argmax(model.predict(obs))\n",
    "        next_state, reward, done, _ = env._step(act)\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        obs = next_state  \n",
    "\n",
    "    scores_arr.append(ep_reward)\n",
    "      \n",
    "print(\"Mean over 100 episodes: {}\".format(round(np.mean(scores_arr),1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
